While first and second moments give us some idea of the performance of our sampling algorithms, we ideally would like a fuller picture.  In this section we compare the performance of algorithms using the total variation distance, $L^2$-Wasserstein distance and Kullback--Leibler divergence.  Using these measures, we can compare the performance to theoretical upper bounds for the \texttt{ULA}, \texttt{tULA} and \texttt{HOLA} algorithms.

\subsection{Statistical Distances}
Let $\mathcal{B}(\R^d)$ denote the Borel $\sigma$-algebra on $\R^d$. Let $P$ and $Q$ be probability measures on the space $(\R^d, \mathcal{B}(\R^d))$.  Then we define the total variation distance, Kullback--Leibler divergence and Wasserstein metric as follows:

\begin{defn}[Total Variation]
The total variation distance between two probability measures $P$ and $Q$ on $(\Omega, \mathcal{F})$ is defined as
$$
\norm{P - Q}_{TV} = \sup_{A \in \mathcal{F}} \abs{P(A) - Q(A)}.
$$
\end{defn}
\begin{prop}
If the set $\Omega$ is countable then this is equivalent to half the $L^1$ norm.
$$
\norm{P - Q}_{TV} = \frac{1}{2} \norm{P-Q}_1 = \frac{1}{2} \sum_{\omega \in \Omega} \abs{P(\omega) - Q(\omega)}
$$
\end{prop}
\begin{proof}
Let $B = \{\omega: P(\omega) \geq Q(\omega)\}$ and let $A \in \mathcal{F}$ be any event.  Then
$$
P(A) - Q(A) \leq P(A \cap B) - Q(A \cap B) \leq P(B) - Q(B).
$$
The first inequality holds since $P(\omega)-Q(\omega) < 0$ for any $\omega \in A \cap B^c$, and so the difference in probability cannot be greater if these elements are excluded.  For the second inequality, we observe that including further elements of $B$ cannot decrease the difference in probability.
Similarly,
$$
Q(A) - P(A) \leq Q(B^c) - P(B^c) = P(B) - Q(B)
$$
Thus, setting $A=B$, we have that $\abs{P(A)-Q(A)}$ is equal to the upper bound in the total variation distance.  Hence,
$$
\norm{P-Q}_{TV} = \frac{1}{2} \abs{P(B)-Q(B)+Q(B^c)-P(B^c)} = \frac{1}{2} \sum_{\omega \in \Omega} \abs{P(x)-Q(x)}
$$
\end{proof}

\begin{defn}[Kullback--Leibler Divergence]
Let $P$ and $Q$ be two probability measures on $(\Omega, \mathcal{F})$.  If $P \ll Q$, the Kullback--Leibler divergence of $P$ with respect to $Q$ is defined as
$$
KL(P|Q) = \int_\Omega \frac{d P}{d Q} log \left(  \frac{d P}{d Q} \right) d Q.
$$
\end{defn}
Finally we consider the Wasserstein distance.  If $P$ and $Q$ are probability measures on $(\R^d,\mathcal{B}(\R^d)$, we say that $\gamma$ is a transport plan between two probability measures $P$ and $Q$ if it is a probability measure on $(\R^d \times \R^d, \mathcal{B}(\R^d \times \R^d))$ such that for any Borel set $A \subset \R^d$, $\gamma(A \times \R^d)=P(A)$ and $\gamma(\R^d \times A) = Q(A)$.  We denote the set of all such transport plans by $\Pi(P,Q)$.
\begin{defn}[Wasserstein distance]
For two probability measures, $P$ and $Q$, the $L^p$-Wasserstein distance is given by
$$
W_p(P,Q) = \left( \inf_{\gamma \in \Pi(P,Q)} \int_{\R^d \times \R^d} \norm{x-y}^p d \gamma(x,y) \right)^{1/p}.
$$
\end{defn}
We will restrict our attention here to the $L^2$-Wasserstein distance.

\textbf{REMEMBER TO INCLUDE} For 1D $W_p^p$ is equivalent to $\int_0^1 \abs{F^{-1}(t)-G^{-1}(t)}^p dt$ where $F$ and $G$ are the CDFs of $P$ and $Q$ respectively \cite{ramdas2017wasserstein}.

\subsection{Theoretical Bounds}
\subsection{Implementation}
