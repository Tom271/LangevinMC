\section{Langevin Equation}
The Langevin equation is an SDE with the form 
\[\dif X_t = -\nabla U(X_t)\dif t +\sqrt{2\gamma}\dif W_t\]
We call \(U\) the potential function. It is the properties of this function that govern whether each scheme will converge? This equation has \(\pi\) as its invariant distribution, meaning that simply simulating the SDE for long enough will give an approximation of \(\pi\). The obvious way to do this is using the Euler-Maruyama scheme, which gives the following approximation
\[X_{n+1} = X_n -\gamma \nabla U(X_n) +\sqrt{2\gamma} Z_{n+1},\qquad X_0= x_0 \]
Here \(Z_n \sim N(0,1)\) i.i.d. However, it is well known that this method does not always give the correct answer, and diverges whenever \(\nabla U(x)\) is superlinear (See Roberts \& Tweedie 1996). How can we solve this problem? We can look at either discretising the SDE in a different way (HOLA, LM); or we can modify the Euler scheme to mitigate the issues. Here we will focus on the latter, although our code has algorithms from both approaches. 

\section{Taming}
Taming has been suggested for ULA by Brosse, Durmus, Moulines and Sabanis, as well as by Roberts \& Tweedie for MALA (they called the result MALTA). The idea is to scale the gradient 

\section{ULA}
\[X_{n+1} = X_n -\gamma \nabla U(X_n) +\sqrt{2\gamma} Z_{n+1},\qquad X_0= x_0 \]
\section{MALA}
\cite{RT_MALA}
Propose \(V_{n+1}\) using Langevin dynamics:
\[V_{n+1} = X_n -\gamma \nabla U(X_n) +\sqrt{2\gamma} Z_{n+1}\]
Calculate acceptance probability
\[\alpha(X_n,V_{n+1}) = 1\wedge \frac{\pi(V_{n+1})q(V_{n+1},X_n)}{\pi(X_n)q(X_n,V_{n+1})}\]
Here \(q(x,y)\) is the transition probability, \(\P(X_{n+1}=y | X_{n}=x)\). If \texttt{rand}\(\leq\alpha\), 
\[X_{n+1} = V_{n+1}.\]
That is,
\[X_{n+1} = \mathbb{I}(u\leq \alpha)V_{n+1} +\mathbb{I}(u > \alpha)X_n \]

\section{Taming the Gradient}
\subsection{tULA/c}
\[X_{n+1} = X_n -\gamma T_{\gamma}(X_n) +\sqrt{2\gamma} Z_{n+1},\qquad X_0= x_0 \]
where \(T_{\gamma}(x) = \frac{\grad U(x)}{1+\|\grad U(x)\|}\) or \(T_{\gamma}(x) =\left(\frac{\grad U(x)}{1+|\partial_i U(x)|}\right)_{i=\lbrace 1, \dots, d\rbrace} \)
\subsection{tHOLA}
\cite{tHOLA}
Use an It\^o-Taylor expansion 
\[X_{n+1} - X_n + \mu_{\gamma}(X_n)\gamma +\sigma_{\gamma}(X_n)\sqrt{\gamma}Z_{n+1}\]
where 
\[\mu_{\gamma}(x) = -\grad U_{\gamma}(x) +\frac{\gamma}{2}\left( \left( \grad^2U\grad U\right)_{\gamma}(x) - \vec{\Delta}(\grad U)_{\gamma}(x)\right) ,\]
and \(\sigma_{\gamma}(x) = \text{diag}\left(\left( \sigma_{\gamma}^{(k)}(x)\right)_{k\in \lbrace 1,\dots,d\rbrace}\right)\) with,
\[\sigma_{\gamma}^{(k)}(x) = \sqrt{2+\frac{2\gamma^2}{3}\sum_{j=1}^d |\grad^2 U_{\gamma}^{(k,j)}(x)|^2 - 2\gamma \grad^2 U_{\gamma}^{(k,k)}(x)}\]

ALSO need to define the gamma subscript, i.e. the tamed variables. is gamma best subscript? Although fn depends on gamma it doesn't indicate that taming has occurred.
\subsection{tMALA/c}
Use the same taming \(T\) as in \texttt{tULA}. Is this sensible? Could compare with \texttt{MALTA}.
\subsection{MALTA?}\cite{MALTA}
Tame with 
\[T = \frac{ \grad U(x)}{1\vee \gamma |\grad U(x)|}\]
for some constant \(D>0\)
\section{LM}
\cite{LM}
Non-Markovian scheme,
\[X_{n+1} = X_n +\gamma \grad U(X_n) + \sqrt{\frac{\gamma}{2}} \left(Z_n + Z_{n+1}\right) \]
\section{RWM}
Popular variant of the Metropolis-Hastings algorithm (CITE) with a normal proposal. 
\[U_{n+1} = X_n + \sqrt{2\gamma} Z_{n+1}\]

Calculate acceptance probability
\[\alpha(X_n,U_{n+1}) = 1\wedge \frac{\pi(U_{n+1})q(U_{n+1},X_n)}{\pi(X_n)q(X_n,U_{n+1})}\]
Here \(q(x,y)\) is the transition probability, \(\P(X_{n+1}=y | X_{n}=x)\). If \texttt{rand}\(\leq\alpha\), 
\[X_{n+1} = U_{n+1}.\]
That is,
\[X_{n+1} = \mathbb{I}(u\leq \alpha)U_{n+1} +\mathbb{I}(u > \alpha)X_n \] 

