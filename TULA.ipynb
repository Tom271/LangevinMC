{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TULA Code from https://github.com/nbrosse/TULA/blob/master/code_TULA.py \n",
    "##### Tamed Unadjusted Langevin Algorithm from Brosse et al. 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class potential:\n",
    "    \"\"\" Implements the potentials U and the algorithms described in the article.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Parameters for Ginzburg-Landau model\n",
    "    tau=2.0\n",
    "    lamb=0.5\n",
    "    alpha=0.1\n",
    "    # threshold for stopping the trajectory of ULA\n",
    "    threshold=10**5\n",
    "    def __init__(self, typ, d):\n",
    "        \"\"\" Initialize the object.\n",
    "        :param typ: potential U\n",
    "        :param d: dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.type = typ\n",
    "        self.d = d\n",
    "        self.acc_mala = 0 # acceptance probability for MALA \n",
    "        self.acc_rwm = 0 # acceptance probability for RWM\n",
    "        self.acc_tmala = 0 # acceptance probability for TMALA\n",
    "        self.acc_tmalac = 0 # acceptance probability for TMALAc\n",
    "        \n",
    "    def potential(self, X):\n",
    "        \"\"\" Definition of the potential\n",
    "        :param X: point X of Rd where the potential is evaluated\n",
    "        :return: U(X)\n",
    "        \"\"\"    \n",
    "        if self.type == \"ill-cond-gaussian\":\n",
    "            d = self.d\n",
    "            v = np.array(np.arange(1,d+1), dtype = float) \n",
    "            InvSigma = 1. / v\n",
    "            return 0.5*np.dot(X, np.multiply(InvSigma, X))\n",
    "        \n",
    "        elif self.type == \"double-well\":\n",
    "            return (1./4)*np.linalg.norm(X)**4 - (1./2)*np.linalg.norm(X)**2\n",
    "        \n",
    "        elif self.type == \"Ginzburg-Landau\":\n",
    "            tau = self.tau\n",
    "            alpha = self.alpha\n",
    "            lamb = self.lamb\n",
    "            dim = int(np.rint((self.d)**(1./3))) #rint -> round to nearest integer\n",
    "            X = np.reshape(X, (dim,dim,dim)) #Make X into 3d array R^(d x d x d)\n",
    "            temp = np.linalg.norm(np.roll(X, -1, axis=0) - X)**2 \\\n",
    "                   + np.linalg.norm(np.roll(X, -1, axis=1) - X)**2 \\\n",
    "                   + np.linalg.norm(np.roll(X, -1, axis=2) - X)**2\n",
    "            return 0.5*(1-tau)*np.linalg.norm(X)**2 + 0.5*tau*alpha*temp \\\n",
    "                    + (1./4)*tau*lamb*np.sum(np.power(X,4))\n",
    "        else:\n",
    "            print(\"Error potential not defined\")\n",
    "            \n",
    "    def gradpotential(self, X):\n",
    "        \"\"\" Definition of the gradient of the potential\n",
    "        :param X: point X of Rd where the gradient is evaluated\n",
    "        :return: gradU(X)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.type == \"ill-cond-gaussian\":\n",
    "            d = self.d\n",
    "            v = np.array(np.arange(1,d+1), dtype = float) \n",
    "            InvSigma = 1. / v\n",
    "            return np.multiply(InvSigma, X)\n",
    "        elif self.type == \"double-well\":\n",
    "            return (np.linalg.norm(X)**2 - 1)*X\n",
    "        elif self.type == \"Ginzburg-Landau\":\n",
    "            tau = self.tau\n",
    "            alpha = self.alpha\n",
    "            lamb = self.lamb\n",
    "            dim = int(np.rint((self.d)**(1./3)))\n",
    "            X = np.reshape(X, (dim,dim,dim))\n",
    "            temp = np.roll(X, 1, axis=0)+np.roll(X, -1, axis=0) \\\n",
    "                    +np.roll(X, 1, axis=1)+np.roll(X, -1, axis=1) \\\n",
    "                    +np.roll(X, 1, axis=2)+np.roll(X, -1, axis=2)\n",
    "            gradU = (1.0-tau)*X + tau*lamb*np.power(X,3) + tau*alpha*(6*X - temp)\n",
    "            return gradU.flatten()\n",
    "        else:\n",
    "            print(\"Error gradpotential not defined\")\n",
    "        def grad2potential(self,X): \n",
    "            \"\"\" Definition of the gradient of the potential\n",
    "            :param X: point X of Rd where the gradient is evaluated\n",
    "            :return: gradU(X)\n",
    "            \"\"\"\n",
    "        if self.type == \"double-well\":\n",
    "            return ((np.linalg.norm(X)**2 - 1) * np.identity(self.d) + 2 * np.transpose(np.matrix(X)) * np.matrix(X))\n",
    "    \n",
    "    def vectorLaplaciangradpotential(self,X):\n",
    "        if self.type == \"double-well\":\n",
    "            return(6*X)\n",
    "\n",
    "    \n",
    "    def tHOLA(self, x0, step, N=10**6):\n",
    "        \"\"\"Tamed Higher order Langevin Algorithm (Sabanis & Zhang)\n",
    "        \"\"\"\n",
    "        d = self.d\n",
    "        X = x0\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        burnin = 10**4 # burn in period\n",
    "        for k in np.arange(burnin):\n",
    "            if np.linalg.norm(X, np.inf)>self.threshold:\n",
    "                m1[:] = np.nan\n",
    "                m2[:] = np.nan\n",
    "                return (m1,m2)\n",
    "            gradU = self.gradpotential(X)\n",
    "            gradUgamma = gradU / ((1 + step ** (3 / 2) * np.linalg.norm(gradU) ** (3/2)) ** (2/3))\n",
    "            grad2U = self.grad2potential(X)\n",
    "            grad2Ugamma = grad2U/(1 + step * np.linalg.norm(grad2U))\n",
    "            laplacianGradU = self.vectorLaplaciangradpotential(X)\n",
    "            laplacianGradUgamma = laplacianGradU/(1 + step ** (1/2) * np.linalg.norm(X) * np.linalg.norm(laplacianGradU))\n",
    "            gradUgrad2Ugamma = (gradU * grad2U) / (1 + step * np.linalg.norm(X) * np.linalg.norm(grad2U) * np.linalg.norm(gradU))\n",
    "            X += - step*gradUgamma + (step**2)/2 * (gradUgrad2Ugamma - laplacianGradUgamma) + np.sqrt(2*step)*np.random.normal(size=d) - np.sqrt(2) * grad2Ugamma * np.sqrt(step ** 3 / 3) * np.random.normal(size=d)\n",
    "        for k in np.arange(N):\n",
    "            if np.linalg.norm(X, np.inf)>self.threshold:\n",
    "                m1[:] = np.nan\n",
    "                m2[:] = np.nan\n",
    "                return (m1,m2)\n",
    "            m1 += X / float(N)\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            gradU = self.gradpotential(X)\n",
    "            gradUgamma = gradU / ((1 + step ** (3 / 2) * np.linalg.norm(gradU) ** (3/2)) ** (2/3))\n",
    "            grad2U = self.grad2potential(X)\n",
    "            grad2Ugamma = grad2U/(1 + step * np.linalg.norm(grad2U))\n",
    "            laplacianGradU = self.vectorLaplaciangradpotential(X)\n",
    "            laplacianGradUgamma = laplacianGradU/(1 + step ** (1/2) * np.linalg.norm(X) * np.linalg.norm(laplacianGradU))\n",
    "            gradUgrad2Ugamma = (gradU * grad2U) / (1 + step * np.linalg.norm(X) * np.linalg.norm(grad2U) * np.linalg.norm(gradU))\n",
    "            X += - step*gradUgamma + (step**2)/2 * (gradUgrad2Ugamma - laplacianGradUgamma) + np.sqrt(2*step)*np.random.normal(size=d) - np.sqrt(2) * grad2Ugamma * np.sqrt(step ** 3 / 3) * np.random.normal(size=d)\n",
    "        return (m1,m2)\n",
    "    \n",
    "    def ULA(self, x0, step, N=10**6):\n",
    "        \"\"\" Algorithm ULA\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm ?gamma in paper?\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :return: empirical averages of 1st and 2nd moments m1,m2\n",
    "        \"\"\"\n",
    "        d = self.d\n",
    "        X = x0\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        burnin = 10**4 # burn in period\n",
    "        for k in np.arange(burnin):\n",
    "            #Check divergence? Max row sum > threshold\n",
    "            if np.linalg.norm(X, np.inf)>self.threshold:\n",
    "                m1[:] = np.nan\n",
    "                m2[:] = np.nan\n",
    "                return (m1,m2)\n",
    "            \n",
    "            gradU = self.gradpotential(X)\n",
    "            #E-M discretization of Langevin SDE, take a step to X_k+1\n",
    "            X = X - step*gradU + np.sqrt(2*step)*np.random.normal(size=d)\n",
    "        #Same again but after burn-in\n",
    "        for k in np.arange(N):\n",
    "            if np.linalg.norm(X, np.inf)>self.threshold:\n",
    "                m1[:] = np.nan\n",
    "                m2[:] = np.nan\n",
    "                return (m1,m2)\n",
    "            #Calculate mean\n",
    "            m1 += X / float(N)\n",
    "            #Calculate E[X^2]\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            gradU = self.gradpotential(X)\n",
    "            X = X - step*gradU + np.sqrt(2*step)*np.random.normal(size=d)\n",
    "        return (m1,m2)\n",
    "    \n",
    "    def TULA(self, x0, step, N=10**6):\n",
    "        \"\"\" Algorithm TULA\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :return: empirical averages of 1st and 2nd moments\n",
    "        \"\"\"\n",
    "        #TULA same as ULA but use tamed grad instead of grad\n",
    "        \n",
    "        d = self.d\n",
    "        X = x0\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        burnin = 10**4\n",
    "        for k in np.arange(burnin):\n",
    "            gradU = self.gradpotential(X)\n",
    "            gradUTamed = gradU / (1.0 + step*np.linalg.norm(gradU))\n",
    "            X = X - step*gradUTamed + np.sqrt(2*step)*np.random.normal(size=d)\n",
    "        for k in np.arange(N):\n",
    "            m1 += X / float(N)\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            gradU = self.gradpotential(X)\n",
    "            gradUTamed = gradU / (1.0 + step*np.linalg.norm(gradU))\n",
    "            X = X - step*gradUTamed + np.sqrt(2*step)*np.random.normal(size=d)\n",
    "        return (m1,m2)\n",
    "    \n",
    "    def TULAc(self, x0, step, N=10**6):\n",
    "        \"\"\" Algorithm TULAc\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :return: empirical averages of 1st and 2nd moments\n",
    "        \"\"\"\n",
    "        \n",
    "        d = self.d\n",
    "        X = x0\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        burnin = 10**4\n",
    "        for k in np.arange(burnin):\n",
    "            gradU = self.gradpotential(X)\n",
    "            #Here is the difference: \n",
    "            gradUTamed = np.divide(gradU, 1.0 + step*np.absolute(gradU))\n",
    "            #Does not require norm calculation, only abs value. np.divide is element div\n",
    "            X = X - step*gradUTamed + np.sqrt(2*step)*np.random.normal(size=d)\n",
    "        for k in np.arange(N):\n",
    "            m1 += X / float(N)\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            gradU = self.gradpotential(X)\n",
    "            gradUTamed = np.divide(gradU, 1.0 + step*np.absolute(gradU))\n",
    "            X = X - step*gradUTamed + np.sqrt(2*step)*np.random.normal(size=d)\n",
    "        return (m1,m2)\n",
    "    \n",
    "    \n",
    "    def TMALA(self, x0, step, N=10**6):\n",
    "        \"\"\" Algorithm TMALA\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :return: empirical averages of 1st and 2nd moments\n",
    "        \"\"\"\n",
    "        \n",
    "        acc=0 # to store the empirical acceptance probability\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        X = x0\n",
    "        burnin = 10**4\n",
    "        for k in np.arange(burnin):\n",
    "            U_X = self.potential(X)\n",
    "            #MALA but tame gradU as in TULA\n",
    "            grad_U_X = self.gradpotential(X)\n",
    "            Tgrad_U_X = grad_U_X / (1. + step*np.linalg.norm(grad_U_X))\n",
    "            #Y is proposal\n",
    "            Y = X - step * Tgrad_U_X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            grad_U_Y = self.gradpotential(Y)\n",
    "            Tgrad_U_Y = grad_U_Y / (1. + step*np.linalg.norm(grad_U_Y))\n",
    "            logratio = - U_Y + U_X + (1./(4*step))*(np.linalg.norm(Y-X+step*Tgrad_U_X)**2 \\\n",
    "                           - np.linalg.norm(X-Y+step*Tgrad_U_Y)**2)\n",
    "            #Metropolis rejection\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "        \n",
    "        #Same again after burn-in\n",
    "        for k in np.arange(N):\n",
    "            m1 += X / float(N)\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            U_X = self.potential(X)\n",
    "            grad_U_X = self.gradpotential(X)\n",
    "            Tgrad_U_X = grad_U_X / (1. + step*np.linalg.norm(grad_U_X))\n",
    "            Y = X - step * Tgrad_U_X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            grad_U_Y = self.gradpotential(Y)\n",
    "            Tgrad_U_Y = grad_U_Y / (1. + step*np.linalg.norm(grad_U_Y))\n",
    "            logratio = - U_Y + U_X + (1./(4*step))*(np.linalg.norm(Y-X+step*Tgrad_U_X)**2 \\\n",
    "                           - np.linalg.norm(X-Y+step*Tgrad_U_Y)**2)\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "                acc+=1\n",
    "        self.acc_tmala = float(acc)/N # empirical acceptance probability\n",
    "        return (m1,m2)\n",
    "    \n",
    "    \n",
    "    def TMALAc(self, x0, step, N=10**6):\n",
    "        \"\"\" Algorithm TMALAc\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :return: empirical averages of 1st and 2nd moments\n",
    "        \"\"\"\n",
    "        \n",
    "        acc=0\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        X = x0\n",
    "        burnin=10**4\n",
    "        for k in np.arange(burnin):\n",
    "            U_X = self.potential(X)\n",
    "            grad_U_X = self.gradpotential(X)\n",
    "            Tgrad_U_X = np.divide(grad_U_X, 1. + step*np.absolute(grad_U_X))\n",
    "            Y = X - step * Tgrad_U_X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            grad_U_Y = self.gradpotential(Y)\n",
    "            Tgrad_U_Y = np.divide(grad_U_Y, 1. + step*np.absolute(grad_U_Y))\n",
    "            logratio = - U_Y + U_X + (1./(4*step))*(np.linalg.norm(Y-X+step*Tgrad_U_X)**2 \\\n",
    "                           - np.linalg.norm(X-Y+step*Tgrad_U_Y)**2)\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "        for k in np.arange(N):\n",
    "            m1 += X / float(N)\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            U_X = self.potential(X)\n",
    "            grad_U_X = self.gradpotential(X)\n",
    "            Tgrad_U_X = np.divide(grad_U_X, 1. + step*np.absolute(grad_U_X))\n",
    "            Y = X - step * Tgrad_U_X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            grad_U_Y = self.gradpotential(Y)\n",
    "            Tgrad_U_Y = np.divide(grad_U_Y, 1. + step*np.absolute(grad_U_Y))\n",
    "            logratio = - U_Y + U_X + (1./(4*step))*(np.linalg.norm(Y-X+step*Tgrad_U_X)**2 \\\n",
    "                           - np.linalg.norm(X-Y+step*Tgrad_U_Y)**2)\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "                acc+=1\n",
    "        self.acc_tmalac = float(acc)/N\n",
    "        return (m1,m2)\n",
    "    \n",
    "    \n",
    "    def MALA(self, x0, step, N=10**6):\n",
    "        \"\"\" Algorithm TMALA\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :return: empirical averages of 1st and 2nd moments\n",
    "        \"\"\"\n",
    "        \n",
    "        acc=0\n",
    "        d = self.d\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        X = x0\n",
    "        burnin = 10**4\n",
    "        for k in np.arange(burnin):\n",
    "            U_X = self.potential(X)\n",
    "            grad_U_X = self.gradpotential(X)\n",
    "            Y = X - step * grad_U_X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            grad_U_Y = self.gradpotential(Y)\n",
    "            logratio = - U_Y + U_X + (1./(4*step))*(np.linalg.norm(Y-X+step*grad_U_X)**2 \\\n",
    "                           - np.linalg.norm(X-Y+step*grad_U_Y)**2)\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "        for k in np.arange(N):\n",
    "            m1 += X / float(N)\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            U_X = self.potential(X)\n",
    "            grad_U_X = self.gradpotential(X)\n",
    "            Y = X - step * grad_U_X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            grad_U_Y = self.gradpotential(Y)\n",
    "            logratio = - U_Y + U_X + (1./(4*step))*(np.linalg.norm(Y-X+step*grad_U_X)**2 \\\n",
    "                           - np.linalg.norm(X-Y+step*grad_U_Y)**2)\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "                acc+=1\n",
    "        self.acc_mala = float(acc)/N\n",
    "        return (m1,m2)\n",
    "\n",
    "    def RWM(self, x0, step, N=10**6):\n",
    "        \"\"\" Algorithm RWM\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :return: empirical averages of 1st and 2nd moments\n",
    "        \"\"\"\n",
    "        \n",
    "        acc=0\n",
    "        d = self.d\n",
    "        m1 = np.zeros(d)\n",
    "        m2 = np.zeros(d)\n",
    "        X = x0\n",
    "        burnin = 10**4\n",
    "        for k in np.arange(burnin):\n",
    "            U_X = self.potential(X)\n",
    "            Y = X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            logratio = - U_Y + U_X\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "        for k in np.arange(N):\n",
    "            m1 += X / float(N)\n",
    "            m2 += np.power(X, 2) / float(N)\n",
    "            U_X = self.potential(X)\n",
    "            Y = X + np.sqrt(2*step)*np.random.normal(size=self.d)\n",
    "            U_Y = self.potential(Y)\n",
    "            logratio = - U_Y + U_X\n",
    "            if np.log(np.random.uniform(size=1))<=logratio:\n",
    "                X = Y\n",
    "                acc+=1\n",
    "        self.acc_rwm = float(acc)/N\n",
    "        return (m1,m2)\n",
    "\n",
    "    #Actually running the method\n",
    "    def sampler(self, x0, step, N=10**6, choice=\"ULA\"):\n",
    "        \"\"\" General call to a sampler ULA, TULA, TULAc, MALA, RWM, TMALA or TMALAc\n",
    "        :param x0: starting point\n",
    "        :param step: step size of the algorithm\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :param choice: choice of the algorithm\n",
    "        :return: empirical averages of 1st and 2nd moments\n",
    "        \"\"\"\n",
    "        \n",
    "        if choice==\"ULA\":\n",
    "            return self.ULA(x0, step, N=N)\n",
    "        elif choice==\"TULA\":\n",
    "            return self.TULA(x0, step, N=N)\n",
    "        elif choice==\"TULAc\":\n",
    "            return self.TULAc(x0, step, N=N)\n",
    "        elif choice==\"MALA\":\n",
    "            return self.MALA(x0, step, N=N)\n",
    "        elif choice==\"RWM\":\n",
    "            return self.RWM(x0, step, N=N)\n",
    "        elif choice==\"TMALA\":\n",
    "            return self.TMALA(x0, step, N=N)\n",
    "        elif choice==\"TMALAc\":\n",
    "            return self.TMALAc(x0, step, N=N)\n",
    "        else:\n",
    "            print(\"error sampler not defined\")\n",
    "    \n",
    "    def analysis(self, x0, NSimu=100, N=10**6, \\\n",
    "                     stepTab = np.array([10**(-3), 10**(-2), 10**(-1), 1.0]), \\\n",
    "                     choice=\"ULA\"):\n",
    "        \"\"\" Analysis for one given algorithm, different step sizes, and NSimu\n",
    "        independent simulations \n",
    "        \n",
    "        Warning: this function may need high computational power\n",
    "        \n",
    "        :param x0: starting point\n",
    "        :param NSimu: number of independent simulations\n",
    "        :param N: number of iterations (after burn in period)\n",
    "        :param stepTab: table of different step sizes for the algorithm\n",
    "        :param choice: choice of the algorithm\n",
    "        \n",
    "        :return: algorithm, stepTab, initial condition,\n",
    "        average acceptance probability for MALA, RWM, TMALA and TMALAc,\n",
    "        empirical averages of 1st and 2nd moments for the first and  last coordinate,\n",
    "        the different step sizes and the different independent simulations (NSimu)\n",
    "        \"\"\"\n",
    "        \n",
    "        d = self.d\n",
    "        nStep = stepTab.size\n",
    "        \n",
    "        # Acceptance probabilities\n",
    "        accMala = np.zeros(nStep)\n",
    "        accRwm = np.zeros(nStep)\n",
    "        accTmala = np.zeros(nStep)\n",
    "        accTmalac = np.zeros(nStep)\n",
    "        \n",
    "        # To store the empirical evrages of 1st and 2nd moments for the\n",
    "        # first and last coordinate\n",
    "        moment_1 = np.zeros((nStep, NSimu, 2))\n",
    "        moment_2 = np.zeros((nStep, NSimu, 2))\n",
    "        \n",
    "        for i in np.arange(nStep):\n",
    "            m1tab = np.zeros((NSimu, d))\n",
    "            m2tab = np.zeros((NSimu, d))\n",
    "            for k in np.arange(NSimu):\n",
    "                m1,m2 = self.sampler(x0, step=stepTab[i], N=N, choice=choice)\n",
    "                m1tab[k,:] = m1\n",
    "                m2tab[k,:] = m2\n",
    "                accMala[i] += float(self.acc_mala)/NSimu\n",
    "                accRwm[i] += float(self.acc_rwm)/NSimu\n",
    "                accTmala[i] += float(self.acc_tmala)/NSimu\n",
    "                accTmalac[i] += float(self.acc_tmalac)/NSimu      \n",
    "            moment_1[i, :] = m1tab[:,[0,-1]]\n",
    "            moment_2[i, :] = m2tab[:,[0,-1]]\n",
    "        \n",
    "        if choice==\"MALA\":\n",
    "            return (choice, stepTab, x0, accMala, moment_1, moment_2)\n",
    "        elif choice==\"RWM\":\n",
    "            return (choice, stepTab, x0, accRwm, moment_1, moment_2)\n",
    "        elif choice==\"TMALA\":\n",
    "            return (choice, stepTab, x0, accTmala, moment_1, moment_2)\n",
    "        elif choice==\"TMALAc\":\n",
    "            return (choice, stepTab, x0, accTmalac, moment_1, moment_2)\n",
    "        else:            \n",
    "            return (choice, stepTab, x0, moment_1, moment_2)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TULA',\n",
       " array([0.001, 0.01 , 0.1  , 1.   ]),\n",
       " array([0, 0]),\n",
       " array([[[-0.34936666,  0.54244401],\n",
       "         [ 0.22453253,  0.15256812]],\n",
       " \n",
       "        [[-0.12348253,  0.13929409],\n",
       "         [ 0.00212183,  0.01203119]],\n",
       " \n",
       "        [[-0.01274336, -0.01336027],\n",
       "         [-0.048388  ,  0.05484786]],\n",
       " \n",
       "        [[ 0.091485  ,  0.07067956],\n",
       "         [ 0.10423079, -0.07006242]]]),\n",
       " array([[[0.66400085, 0.9681286 ],\n",
       "         [0.71889292, 0.86245103]],\n",
       " \n",
       "        [[0.75303877, 0.77273896],\n",
       "         [0.82045408, 0.73408484]],\n",
       " \n",
       "        [[0.91235492, 0.93585473],\n",
       "         [0.92592257, 0.91001729]],\n",
       " \n",
       "        [[4.96993939, 5.24493509],\n",
       "         [4.74841755, 4.49867706]]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pot=potential(\"double-well\",2)\n",
    "\n",
    "pot.analysis(np.array([0,0]),NSimu=2,N=10**4,choice=\"TULA\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
