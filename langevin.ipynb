{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import normal, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "from time import clock\n",
    "\n",
    "class Potential:\n",
    "    \"\"\" Represents a potential function. \"\"\"\n",
    "    def __init__(self, potential=\"gaussian\", dimension=1):\n",
    "        self.dim = dimension\n",
    "        self.name = potential\n",
    "        # To add a potential, add it to the dictionary below and implement it/its gradient.\n",
    "        self.function, self.gradient, self.gradient2, self.vector_lap_grad = {\n",
    "            \"gaussian\":         (self.gaussian,         self.gaussian_grad,         None, None),\n",
    "            \"double_well\":      (self.double_well,      self.double_well_grad,      self.double_well_grad2, self.double_well_vector_lap_grad),\n",
    "            \"Ginzburg_Landau\":  (self.Ginzburg_Landau,  self.Ginzburg_Landau_grad,  None, None)\n",
    "        }[potential]\n",
    "\n",
    "        # Quantities to store in the Potential class, to avoid needless re-computation.\n",
    "        self.inv_sigma = 1. / np.arange(1, dimension+1, dtype=float) # for Gaussian\n",
    "\n",
    "    def gaussian(self, x):\n",
    "        return 0.5 * np.dot(x, np.multiply(self.inv_sigma, x))\n",
    "\n",
    "    def gaussian_grad(self, x):\n",
    "        return np.multiply(self.inv_sigma, x)\n",
    "\n",
    "    def double_well(self, x):\n",
    "        normx = norm(x)\n",
    "        return 0.25 * normx**4 - 0.5 * normx**2\n",
    "\n",
    "    def double_well_grad(self, x):\n",
    "        return (norm(x)**2 - 1) * x\n",
    "\n",
    "    def double_well_grad2(self, x):\n",
    "        mx = np.matrix(x)\n",
    "        return (norm(x)**2 - 1) * np.identity(self.dim) + 2 * np.transpose(mx) * mx\n",
    "\n",
    "    def double_well_vector_lap_grad(self, x):\n",
    "        return 6*x\n",
    "\n",
    "    def Ginzburg_Landau(self, x, tau=2.0, lamb=0.5, alpha=0.1):\n",
    "        d_ = round(self.dim ** (1./3))\n",
    "        x = np.reshape(x, (d_,d_,d_))\n",
    "        nabla_tilde = sum( norm(np.roll(x, -1, axis=a) - x)**2 for a in [0,1,2] )\n",
    "        return 0.5 * (1. - tau) * norm(x)**2 + \\\n",
    "               0.5 * tau * alpha * nabla_tilde + \\\n",
    "               0.25 * tau * lamb * np.sum(np.power(x, 4))\n",
    "\n",
    "    def Ginzburg_Landau_grad(self, x, tau=2.0, lamb=0.5, alpha=0.1):\n",
    "        d_ = round(self.dim ** (1./3))\n",
    "        x = np.reshape(x, (d_,d_,d_))\n",
    "        temp = sum( np.roll(x, sgn, axis=a) for sgn in [-1,1] for a in [0,1,2] )\n",
    "        return ((1. - tau) * x + \\\n",
    "                tau * lamb * np.power(x, 3) + \\\n",
    "                tau * alpha * (6*x - temp)).flatten()\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\" Evaluates a set of Langevin algorithms on given potentials. \"\"\"\n",
    "    def __init__(self, potential=\"gaussian\", dimension=1, N=10**2, burn_in=10**2, N_sim=5, x0=[0], step=0.01, timer=None):\n",
    "        self.dim = dimension\n",
    "        # To add an algorithm, add it to the dictionary below and implement it as a class method.\n",
    "        self.algorithms = {\n",
    "            \"ULA\":     self.ULA,\n",
    "            \"tULA\":    self.tULA,\n",
    "            \"tULAc\":   self.tULAc,\n",
    "            \"MALA\":    self.MALA,\n",
    "            \"RWM\":     self.RWM,\n",
    "            \"tMALA\":   self.tMALA,\n",
    "            \"tMALAc\":  self.tMALAc,\n",
    "            \"tHOLA\":   self.tHOLA,\n",
    "            \"LM\":      self.LM,\n",
    "            \"tLM\":     self.tLM,\n",
    "            \"tLMc\":    self.tLMc\n",
    "        }\n",
    "        self.N = N\n",
    "        self.burn_in = burn_in\n",
    "        self.N_sim = N_sim\n",
    "        self.x0 = x0\n",
    "        self.step = step\n",
    "        self.timer = timer\n",
    "        if timer:\n",
    "            self.N = 10**10 # ~~practically infinity\n",
    "            self.start_time = clock()\n",
    "        self.potential = Potential(potential, dimension)\n",
    "        # invoked by self.potential.funtion(parameters), self.potential.gradient(parameters)\n",
    "\n",
    "    def ULA(self):\n",
    "        x = np.array(self.x0)\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        sqrtstep = np.sqrt(2*self.step)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "            x += -self.step * self.potential.gradient(x) + sqrtstep * normal(size=self.dim)\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1 #i+1 = no. of iterations\n",
    "\n",
    "    def tULA(self, taming=(lambda g, step: g/(1. + step*norm(g)))):\n",
    "        x = np.array(self.x0)\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        sqrtstep = np.sqrt(2*self.step)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "            x += -self.step * taming(self.potential.gradient(x), self.step) + sqrtstep * normal(size=self.dim)\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1 #i+1 = no. of iterations\n",
    "\n",
    "    def tULAc(self):\n",
    "        # coordinate-wise taming function\n",
    "        return self.tULA(lambda g, step: np.divide(g, 1. + step*np.absolute(g)))\n",
    "\n",
    "    def MALA(self):\n",
    "        acc = 0 # acceptance probability\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        x = np.array(self.x0)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "            U_x, grad_U_x = self.potential.function(x), self.potential.gradient(x)\n",
    "            y = x - self.step * grad_U_x + np.sqrt(2 * self.step) * normal(size=self.dim)\n",
    "            U_y, grad_U_y = self.potential.function(y), self.potential.gradient(y)\n",
    "\n",
    "            logratio = -U_y + U_x + 1./(4*self.step) * (norm(y - x + self.step*grad_U_x)**2 \\\n",
    "                       -norm(x - y + self.step*grad_U_y)**2)\n",
    "            if np.log(uniform(size = 1)) <= logratio:\n",
    "                x = y\n",
    "                if i >= self.burn_in:\n",
    "                    acc += 1\n",
    "\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1, acc/float(i+1) #i+1 = no. of iterations\n",
    "\n",
    "    def RWM(self):\n",
    "        acc = 0 # acceptance probability\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        x = np.array(self.x0)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "            y = x + np.sqrt(2*self.step) * normal(self.dim)\n",
    "            logratio = self.potential.function(x) - self.potential.function(y)\n",
    "            if np.log(uniform(size = 1)) <= logratio:\n",
    "                x = y\n",
    "                if i >= self.burn_in:\n",
    "                    acc += 1\n",
    "\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1, acc/float(i+1) #i+1 = no. of iterations\n",
    "\n",
    "\n",
    "    def tMALA(self, taming=(lambda g, step: g/(1. + step*norm(g)))):\n",
    "        acc = 0 # acceptance probability\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        x = np.array(self.x0)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "            U_x, grad_U_x = self.potential.function(x), self.potential.gradient(x)\n",
    "            tamed_gUx = taming(grad_U_x, self.step)\n",
    "            y = x - self.step * tamed_gUx + np.sqrt(2*self.step) * normal(size=self.dim)\n",
    "            U_y, grad_U_y = self.potential.function(y), self.potential.gradient(y)\n",
    "            tamed_gUy = taming(grad_U_y, self.step)\n",
    "\n",
    "            logratio = -U_y + U_x + 1./(4*self.step) * (norm(y - x + self.step*tamed_gUx)**2 - norm(x - y + self.step*tamed_gUy)**2)\n",
    "            if np.log(uniform(size = 1)) <= logratio:\n",
    "                x = y\n",
    "                if i >= self.burn_in:\n",
    "                    acc += 1\n",
    "\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1, acc/float(i+1) #i+1 = no. of iterations\n",
    "\n",
    "    def tMALAc(self):\n",
    "        return self.tMALA(lambda g, step: np.divide(g, 1. + step * np.absolute(g)))\n",
    "\n",
    "    def tHOLA(self):\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        x = np.array(self.x0)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "\n",
    "            norm_x = norm(x)\n",
    "            grad_U = self.potential.gradient(x)\n",
    "            norm_grad_U = norm(grad_U)\n",
    "            grad_U_gamma = grad_U / (1 + (self.step * norm_grad_U)**1.5)**(2./3)\n",
    "            grad2_U = self.potential.gradient2(x)\n",
    "            norm_grad2_U = norm(grad2_U)\n",
    "            grad2_U_gamma = grad2_U / (1 + self.step * norm_grad2_U)\n",
    "\n",
    "            laplacian_grad_U = self.potential.vector_lap_grad(x)\n",
    "            laplacian_grad_U_gamma = laplacian_grad_U / (1 + self.step**0.5 * norm_x * norm(laplacian_grad_U))\n",
    "\n",
    "            grad2_U_grad_U_gamma = np.matmul(grad2_U, grad_U).A1 / (1 + self.step * norm_x * norm_grad2_U * norm_grad_U)\n",
    "\n",
    "            x += -self.step * grad_U_gamma + 0.5 * self.step**2 * (grad2_U_grad_U_gamma - laplacian_grad_U_gamma) + \\\n",
    "                  np.sqrt(2*self.step) * normal(size=self.dim) - np.sqrt(2) * np.matmul(grad2_U_gamma, normal(size=self.dim)).A1 * np.sqrt(self.step**3/3)\n",
    "\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1 #i+1 = no. of iterations\n",
    "\n",
    "    def LM(self):\n",
    "        x = np.array(self.x0)\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        sqrtstep = np.sqrt(0.5 * self.step)\n",
    "        gaussian = normal(size=self.dim)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "            gaussian_plus1 = normal(size=self.dim)\n",
    "            x += -self.step * self.potential.gradient(x) + sqrtstep * (gaussian + gaussian_plus1) * 0.5\n",
    "            gaussian = gaussian_plus1\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1 #i+1 = no. of iterations\n",
    "\n",
    "    def tLM(self, taming=(lambda g, step: g/(1. + step * norm(g)))):\n",
    "        x = np.array(self.x0)\n",
    "        m1, m2 = np.zeros(self.dim, dtype=float), np.zeros(self.dim, dtype=float)\n",
    "        sqrtstep = np.sqrt(0.5 * self.step)\n",
    "        gaussian = normal(size=self.dim)\n",
    "\n",
    "        for i in range(self.burn_in + self.N):\n",
    "            if self.timer and clock() - self.start_time > self.timer:\n",
    "                break\n",
    "\n",
    "            if i >= self.burn_in:\n",
    "                m1 += x\n",
    "                m2 += np.power(x, 2)\n",
    "            gaussian_plus1 = normal(size=self.dim)\n",
    "            x += -self.step * taming(self.potential.gradient(x), self.step) + sqrtstep * (gaussian + gaussian_plus1) * 0.5\n",
    "            gaussian = gaussian_plus1\n",
    "        return m1/float(i+1-self.burn_in), m2/float(i+1-self.burn_in), i+1 #i+1 = no. of iterations\n",
    "\n",
    "    def tLMc(self):\n",
    "        return self.tLM(lambda g, step: np.divide(g, 1. + step * np.absolute(g)))\n",
    "\n",
    "    def sampler(self, algorithm=\"ULA\"):\n",
    "        if self.timer:\n",
    "            self.start_time = clock()\n",
    "        return self.algorithms[algorithm]()\n",
    "\n",
    "    def analysis(self, algorithm=\"ULA\"):\n",
    "        print(f'\\n######### Algorithm: {algorithm}')\n",
    "        print(f'Dimension: {self.dim}\\nN: {self.N}\\nBurn in: {self.burn_in}\\nNumber of simulations: {self.N_sim}\\nStep: {self.step}\\nTimer: {self.timer}s\\nx0[0]: {self.x0[0]}')\n",
    "        print('#################\\n')\n",
    "\n",
    "        moment1, moment2, acceptances = [], [], []\n",
    "        for _ in range(self.N_sim):\n",
    "            m1, m2, iterations, *acc = self.sampler(algorithm)\n",
    "            if self.potential.name in ['double_well', 'Ginzburg_Landau']:\n",
    "                if norm(m1, np.inf) > 10:\n",
    "                    m1 = np.nan * m1\n",
    "                if norm(m2, np.inf) > 10:\n",
    "                    m2 = np.nan * m2\n",
    "            print(f'Run of {algorithm} finished, with {iterations} iterations. m1[0], m2[0], acc = {m1[0], m2[0], acc}')\n",
    "            moment1.append(m1)\n",
    "            moment2.append(m2)\n",
    "            if acc:\n",
    "                acceptances.append(acc[0])\n",
    "        return moment1, moment2, acceptances\n",
    "\n",
    "\n",
    "\n",
    "# TODO: RWM is buggy\n",
    "\n",
    "potential = 'gaussian'\n",
    "d = 100\n",
    "N = 10**5\n",
    "burn_in = 10**4\n",
    "N_sim = 5\n",
    "x0 = np.array([0] + [0]*(d-1), dtype=float)\n",
    "step = 0.01\n",
    "\n",
    "# TIMER MODE: number of seconds which we allow the algorithms to run\n",
    "# To run normally without a timer, omit the last parameter\n",
    "timer = 2.5\n",
    "e = Evaluator(potential, dimension=d, N=N, burn_in=burn_in, N_sim=N_sim, x0=x0, step=step, timer=timer)\n",
    "\n",
    "algorithms = ['ULA', 'tULA', 'tULAc', 'tHOLA', 'MALA', 'tMALA', 'tMALAc', 'LM', 'tLM', 'tLMc']\n",
    "\n",
    "# First moments. for Gaussian, we take norm, for well and Ginzburg, we take the first coordinate\n",
    "data = [[norm(p) if potential=='gaussian' else p[0] for p in e.analysis(algo)[0]] for algo in algorithms]\n",
    "\n",
    "plt.title(f'Potential: {potential.upper()}\\nDim={d}, {\"Timer\" if e.timer else \"N\"}={timer if e.timer else N}\\nburn_in={burn_in}, N_sim={N_sim}, step={step}, x0[0]={x0[0]}')\n",
    "plt.boxplot(data, labels=algorithms)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
